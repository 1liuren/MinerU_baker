#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFç»Ÿè®¡è„šæœ¬ï¼ˆå‚æ•°åŒ–ã€å¤šæ–‡ä»¶æ±‡æ€»ï¼‰
ç”¨äºç»Ÿè®¡ä¸€ä¸ªæˆ–å¤šä¸ª JSON åˆ—è¡¨æ–‡ä»¶ä¸­çš„ PDF æ€»æ•°ã€åˆæ ¼æ•°ã€è¯­è¨€åˆ†å¸ƒä¸ä¸åˆæ ¼åŸå› ã€‚
ä»…æ”¯æŒæ ‡å‡† JSON åˆ—è¡¨æ ¼å¼ï¼ˆä¸è§£æ JSONLï¼‰ã€‚
"""

import json
import os
from collections import Counter
import argparse
from pathlib import Path

def analyze_pdf_data(items):
    """å¯¹ä¼ å…¥çš„æ¡ç›®åˆ—è¡¨åšç»Ÿè®¡ã€‚items å¿…é¡»æ˜¯ list[dict]ã€‚"""
    if not isinstance(items, list):
        print("é”™è¯¯ï¼šè¾“å…¥æ•°æ®å¿…é¡»æ˜¯ JSON åˆ—è¡¨")
        return None
    
    # åŸºæœ¬ç»Ÿè®¡
    total_pdfs = len(items)
    qualified_pdfs = sum(1 for item in items if item.get('ok_status') == 'åˆæ ¼')
    unqualified_pdfs = total_pdfs - qualified_pdfs
    
    # è®¡ç®—åˆæ ¼ç‡
    qualification_rate = (qualified_pdfs / total_pdfs * 100) if total_pdfs > 0 else 0
    
    # ç»Ÿè®¡é”™è¯¯ç±»å‹
    error_counter = Counter()
    for item in items:
        if item.get('ok_status') == 'ä¸åˆæ ¼':
            error_dict = item.get('error_dict', {})
            for error_code, error_msg in error_dict.items():
                error_counter[error_msg] += 1
    
    # ç»Ÿè®¡é¡µæ•°åˆ†å¸ƒ
    page_ranges = {
        "1-100": 0,
        "101-200": 0,
        "201-300": 0,
        "301-400": 0,
        "401-500": 0,
    }
    large_page_ranges = {}  # å­˜å‚¨500é¡µä»¥ä¸Šçš„åˆ†å¸ƒ
    
    for item in items:
        page_total = item.get('page_total', 0)
        if page_total <= 0:
            continue
        
        if page_total <= 500:
            range_key = f"{((page_total-1)//100)*100+1}-{((page_total-1)//100+1)*100}"
            page_ranges[range_key] = page_ranges.get(range_key, 0) + 1
        else:
            range_key = f"{((page_total-1)//100)*100+1}-{((page_total-1)//100+1)*100}"
            large_page_ranges[range_key] = large_page_ranges.get(range_key, 0) + 1
    
    # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒä¸æŒ‰è¯­è¨€åˆæ ¼æ•°é‡
    language_counter = Counter()
    qualified_language_counter = Counter()
    for item in items:
        file_path = item.get('file_path', '')
        if '/en/' in file_path:
            lang = 'è‹±æ–‡'
        elif '/zh/' in file_path:
            lang = 'ä¸­æ–‡'
        else:
            lang = 'æœªçŸ¥'

        language_counter[lang] += 1
        if item.get('ok_status') == 'åˆæ ¼':
            qualified_language_counter[lang] += 1
    
    return {
        'total_pdfs': total_pdfs,
        'qualified_pdfs': qualified_pdfs,
        'unqualified_pdfs': unqualified_pdfs,
        'qualification_rate': qualification_rate,
        'error_types': dict(error_counter),
        'language_distribution': dict(language_counter),
        'qualified_by_language': dict(qualified_language_counter),
        'page_distribution': {
            'normal': dict(page_ranges),
            'large': dict(sorted(large_page_ranges.items()))  # æŒ‰é¡µæ•°èŒƒå›´æ’åº
        }
    }

def print_statistics(stats):
    """
    æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    if not stats:
        return
    
    print("="*60)
    print("PDFæ•°æ®ç»Ÿè®¡æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»PDFæ•°é‡: {stats['total_pdfs']:,}")
    print(f"  åˆæ ¼æ•°é‡:   {stats['qualified_pdfs']:,}")
    print(f"  ä¸åˆæ ¼æ•°é‡: {stats['unqualified_pdfs']:,}")
    print(f"  åˆæ ¼ç‡:     {stats['qualification_rate']:.2f}%")
    
    print(f"\nğŸ“š é¡µæ•°åˆ†å¸ƒ:")
    print("  500é¡µä»¥ä¸‹:")
    for range_key, count in stats['page_distribution']['normal'].items():
        if count > 0:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„èŒƒå›´
            percentage = (count / stats['total_pdfs'] * 100) if stats['total_pdfs'] > 0 else 0
            print(f"    {range_key}é¡µ: {count:,} ({percentage:.1f}%)")
    
    if stats['page_distribution']['large']:
        print("\n  500é¡µä»¥ä¸Š:")
        for range_key, count in stats['page_distribution']['large'].items():
            percentage = (count / stats['total_pdfs'] * 100) if stats['total_pdfs'] > 0 else 0
            print(f"    {range_key}é¡µ: {count:,} ({percentage:.1f}%)")

    print(f"\nğŸŒ è¯­è¨€åˆ†å¸ƒ:")
    for lang, count in stats['language_distribution'].items():
        percentage = (count / stats['total_pdfs'] * 100) if stats['total_pdfs'] > 0 else 0
        print(f"  {lang}: {count:,} ({percentage:.1f}%)")
    
    # æŒ‰è¯­è¨€åˆæ ¼æ•°é‡
    if 'qualified_by_language' in stats:
        print(f"\nâœ… æŒ‰è¯­è¨€åˆæ ¼æ•°é‡:")
        print(f"  ä¸­æ–‡åˆæ ¼: {stats['qualified_by_language'].get('ä¸­æ–‡', 0):,}")
        print(f"  è‹±æ–‡åˆæ ¼: {stats['qualified_by_language'].get('è‹±æ–‡', 0):,}")
    
    if stats['error_types']:
        print(f"\nâŒ ä¸åˆæ ¼åŸå› ç»Ÿè®¡:")
        sorted_errors = sorted(stats['error_types'].items(), key=lambda x: x[1], reverse=True)
        for error_msg, count in sorted_errors:
            percentage = (count / stats['unqualified_pdfs'] * 100) if stats['unqualified_pdfs'] > 0 else 0
            print(f"  {error_msg}: {count:,} ({percentage:.1f}%)")
    
    print("="*60)

def read_json_list(path: Path):
    """è¯»å–æ ‡å‡† JSON åˆ—è¡¨æ–‡ä»¶ï¼Œå¤±è´¥è¿”å› Noneã€‚"""
    try:
        data = json.loads(path.read_text(encoding='utf-8'))
        if isinstance(data, list):
            return data
        print(f"è­¦å‘Šï¼š{path} ä¸æ˜¯JSONåˆ—è¡¨ï¼Œå·²è·³è¿‡")
        return None
    except Exception as e:
        print(f"è­¦å‘Šï¼šè¯»å– {path} å¤±è´¥ - {e}")
        return None


def parse_args():
    parser = argparse.ArgumentParser(
        description="ç»Ÿè®¡ä¸€ä¸ªæˆ–å¤šä¸ª data_info.json æ–‡ä»¶ï¼ˆæ ‡å‡† JSON åˆ—è¡¨ï¼‰",
    )
    parser.add_argument(
        "-i", "--input", nargs="+", required=False, default=["data/data_info.json"],
        help="è¾“å…¥JSONè·¯å¾„ï¼Œæ”¯æŒå¤šä¸ªï¼ˆç©ºæ ¼åˆ†éš”ï¼‰æˆ–ç›®å½•ï¼ˆå°†è‡ªåŠ¨å¯»æ‰¾å…¶ä¸­çš„*.jsonï¼‰",
    )
    parser.add_argument(
        "-o", "--output", default="pdf_statistics_report.json",
        help="ç»Ÿè®¡ç»“æœè¾“å‡ºè·¯å¾„ï¼ˆJSONï¼‰",
    )
    parser.add_argument(
        "--glob", action="store_true",
        help="å½“è¾“å…¥ä¸ºç›®å½•æ—¶ï¼Œé€’å½’åŒ¹é…è¯¥ç›®å½•ä¸‹çš„ *.json æ–‡ä»¶",
    )
    return parser.parse_args()


def expand_inputs(inputs, use_glob: bool):
    """å°†è¾“å…¥çš„æ–‡ä»¶/ç›®å½•å±•å¼€ä¸ºå®é™… JSON æ–‡ä»¶åˆ—è¡¨ã€‚"""
    files: list[Path] = []
    for s in inputs:
        p = Path(s)
        if p.is_file():
            files.append(p)
        elif p.is_dir():
            if use_glob:
                files.extend(p.rglob("*.json"))
            else:
                # é glob æ¨¡å¼ä¸‹å–ç›®å½•ä¸­ data_info.jsonï¼ˆè‹¥å­˜åœ¨ï¼‰
                candidate = p / "data_info.json"
                if candidate.exists():
                    files.append(candidate)
        else:
            # å°è¯•æŒ‰é€—å·åˆ†éš”
            parts = [Path(x.strip()) for x in s.split(",") if x.strip()]
            for q in parts:
                if q.exists():
                    files.append(q)
    # å»é‡
    uniq = []
    seen = set()
    for f in files:
        sp = str(f.resolve())
        if sp not in seen:
            uniq.append(f)
            seen.add(sp)
    return uniq


def main():
    args = parse_args()

    files = expand_inputs(args.input, args.glob)
    if not files:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„JSONæ–‡ä»¶")
        return

    print(f"å°†ç»Ÿè®¡ä»¥ä¸‹ {len(files)} ä¸ªæ–‡ä»¶ï¼š")
    for f in files:
        print(f"  - {f}")

    # æ±‡æ€»æ•°æ®
    merged: list[dict] = []
    for f in files:
        data = read_json_list(f)
        if data:
            merged.extend(data)

    if not merged:
        print("é”™è¯¯ï¼šæ²¡æœ‰å¯ç»Ÿè®¡çš„æ•°æ®ï¼ˆæ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼‰")
        return

    stats = analyze_pdf_data(merged)
    if not stats:
        print("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
        return

    print_statistics(stats)

    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    try:
        Path(args.output).write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"\nğŸ“„ è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
    except Exception as e:
        print(f"è­¦å‘Šï¼šä¿å­˜ç»Ÿè®¡æŠ¥å‘Šå¤±è´¥ - {e}")

if __name__ == "__main__":
    main()