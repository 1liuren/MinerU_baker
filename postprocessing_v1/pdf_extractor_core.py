#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFå›¾åƒæˆªå–å™¨ - æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
åªä¿ç•™å‘½ä»¤è¡Œæ‰¹é‡å¤„ç†çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œç§»é™¤æ‰€æœ‰Webç›¸å…³ä»£ç 

åŠŸèƒ½ï¼šæ ¹æ®JSONæ–‡ä»¶ä¸­çš„bboxåæ ‡ä»PDFä¸­æˆªå–å›¾åƒ
ä½œè€…ï¼šAI Assistant
ç‰ˆæœ¬ï¼š2.0.0 (ç®€åŒ–ç‰ˆ)
"""

import os
import json
import fitz  # PyMuPDF
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass
from loguru import logger
import shutil
import time
import fnmatch
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import threading
from tqdm import tqdm

import sys

# ç§»é™¤é»˜è®¤çš„ sinkï¼Œé˜²æ­¢é‡å¤
logger.remove()

# é‡æ–°æŠŠç»ˆç«¯ sink åŠ å›æ¥ï¼Œå¹¶è®¾å®šç­‰çº§
logger.add(sys.stderr, level="INFO") 

"""ä½¿ç”¨ tqdm é»˜è®¤å®ç°ä½œä¸ºè¿›åº¦æ¡"""

# å…¨å±€é…ç½®
MAX_WORKERS = min(multiprocessing.cpu_count(), 32)
PROGRESS_CHECK_INTERVAL = 1

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨
task_status_store = {}
task_lock = threading.Lock()

# å…¨å±€PDFç¼“å­˜ï¼ˆæ–‡ä»¶åstem -> ç»å¯¹è·¯å¾„ï¼‰
PDF_CACHE: dict[str, str] = {}

@dataclass
class TaskProgress:
    """ä»»åŠ¡è¿›åº¦ä¿¡æ¯"""
    task_id: str
    total_books: int = 0
    processed_books: int = 0
    current_book: str = ""
    status: str = "pending"  # pending, running, completed, failed, cancelled
    message: str = ""
    results: List[Dict] = None
    start_time: float = 0
    end_time: float = 0
    checkpoint_file: str = ""
    
    def __post_init__(self):
        if self.results is None:
            self.results = []

@dataclass 
class BookTask:
    """å•æœ¬ä¹¦ç±å¤„ç†ä»»åŠ¡"""
    book_name: str
    json_file_path: str
    pdf_path: Optional[str]
    output_dir: str
    task_id: str
    pdf_base_folder: Optional[str] = None
    include_text: bool = False

@dataclass
class TargetInfo:
    """ç›®æ ‡ä¿¡æ¯"""
    id: str
    type: str
    text: str
    bbox: List[float]
    page_idx: int
    path: Optional[str] = None

@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœ"""
    success: bool
    message: str
    saved_images: List[str]
    total_targets: int
    processed_targets: int

@dataclass
class BatchResult:
    """æ‰¹é‡å¤„ç†ç»“æœ"""
    success: bool
    message: str
    total_books: int
    processed_books: int
    failed_books: List[str]
    results: List[Dict]
    task_id: Optional[str] = None


class PDFImageExtractor:
    """PDFå›¾åƒæˆªå–å™¨"""
    
    def __init__(self, pdf_path: str):
        """åˆå§‹åŒ–PDFå›¾åƒæˆªå–å™¨"""
        self.pdf_path = pdf_path
        self.doc = None
        self._open_pdf()
    
    def _open_pdf(self):
        """æ‰“å¼€PDFæ–‡ä»¶"""
        try:
            self.doc = fitz.open(self.pdf_path)
            logger.debug(f"æˆåŠŸæ‰“å¼€PDFæ–‡ä»¶: {self.pdf_path}")
        except Exception as e:
            raise Exception(f"æ— æ³•æ‰“å¼€PDFæ–‡ä»¶ {self.pdf_path}: {e}")
    
    def extract_image_by_bbox(self, page_idx: int, bbox: List[float], output_path: str) -> bool:
        """æ ¹æ®bboxåæ ‡ä»æŒ‡å®šé¡µæˆªå–å›¾åƒ"""
        try:
            if page_idx >= len(self.doc):
                logger.error(f"é¡µç  {page_idx} è¶…å‡ºèŒƒå›´ï¼ŒPDFæ€»é¡µæ•°: {len(self.doc)}")
                return False
            
            # è·å–æŒ‡å®šé¡µé¢
            page = self.doc[page_idx]
            
            # åˆ›å»ºçŸ©å½¢åŒºåŸŸ
            rect = fitz.Rect(bbox[0], bbox[1], bbox[2], bbox[3])
            
            # è·å–ç¼©æ”¾å› å­ï¼ˆé»˜è®¤2.0ï¼‰
            scale_factor = 2.0
            
            # æˆªå–å›¾åƒ
            pix = page.get_pixmap(clip=rect, matrix=fitz.Matrix(scale_factor, scale_factor))
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜å›¾åƒ
            pix.save(output_path)
            logger.debug(f"æˆåŠŸæˆªå–å›¾åƒ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"æˆªå–å›¾åƒå¤±è´¥: {e}")
            return False
    
    def close(self):
        """å…³é—­PDFæ–‡æ¡£"""
        if self.doc:
            self.doc.close()


def get_target_types() -> set:
    """è·å–ç›®æ ‡ç±»å‹é…ç½®"""
    try:
        from config import get_target_types
        return get_target_types()
    except ImportError:
        # é»˜è®¤ç›®æ ‡ç±»å‹
        return {"interline_equation", "table"}


def parse_json_file(json_path: str) -> Dict:
    """è§£æJSONæ–‡ä»¶"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.debug(f"æˆåŠŸè§£æJSONæ–‡ä»¶: {json_path}")
        return data
    except Exception as e:
        raise Exception(f"æ— æ³•è§£æJSONæ–‡ä»¶ {json_path}: {e}")
def _stem_key(name: str) -> str:
    return (name or "").strip()



def extract_targets_from_json(json_data: Dict, include_text: bool = False) -> List[TargetInfo]:
    """ä»JSONæ•°æ®ä¸­æå–ç›®æ ‡ä¿¡æ¯"""
    targets: List[TargetInfo] = []
    target_types = get_target_types()
    # å¯é€‰å¯ç”¨ text ç±»å‹
    if include_text:
        target_types = set(target_types) | {"text"}

    # æ¯é¡µæ¯ç±»å‹çš„é€’å¢è®¡æ•°ï¼Œå½“ä¸­é—´JSONç¼ºå°‘ index æ—¶å›é€€ä½¿ç”¨
    page_type_counters: dict[tuple[int, str], int] = {}

    try:
        if 'pdf_info' in json_data:
            pdf_info = json_data['pdf_info']

            for page_idx, page_data in enumerate(pdf_info):
                if 'para_blocks' not in page_data:
                    continue

                for block in page_data['para_blocks']:
                    # å¤„ç†ä¸¤ç§ç»“æ„ï¼šblock å†…ç›´æ¥å« linesï¼Œæˆ–å« blocks[nested] å†å« lines
                    if 'blocks' in block and isinstance(block['blocks'], list):
                        for nested_block in block['blocks']:
                            lines_list = nested_block.get('lines', []) if isinstance(nested_block, dict) else []
                            block_index = nested_block.get('index') if isinstance(nested_block, dict) else None
                            block_type = nested_block.get('type') if isinstance(nested_block, dict) else None

                            for line_idx, line in enumerate(lines_list or []):
                                spans = line.get('spans') if isinstance(line, dict) else None
                                if not spans:
                                    continue

                                # å…ˆè¾“å‡ºè¡¨æ ¼ï¼ˆå— target_types æ§åˆ¶ï¼‰
                                for s_idx, s in enumerate(spans):
                                    if not isinstance(s, dict):
                                        continue
                                    if s.get('type') == 'table' and 'bbox' in s and ('table' in target_types):
                                        targets.append(
                                            TargetInfo(
                                                id=f"page_{page_idx}_span_table_{block_index}",
                                                type='span_table',
                                                text=s.get('html', ''),
                                                bbox=s['bbox'],
                                                page_idx=page_idx,
                                            )
                                        )

                                # è¡Œé—´å…¬å¼ï¼ˆå— target_types æ§åˆ¶ï¼‰ï¼Œå•ç‹¬è¾“å‡ºï¼Œä¸å‚ä¸åˆå¹¶
                                for s_idx, s in enumerate(spans):
                                    if not isinstance(s, dict):
                                        continue
                                    if s.get('type') == 'interline_equation' and 'bbox' in s and ('interline_equation' in target_types):
                                        targets.append(
                                            TargetInfo(
                                                id=f"page_{page_idx}_span_interline_equation_{block_index}",
                                                type='span_interline_equation',
                                                text=s.get('content', ''),
                                                bbox=s['bbox'],
                                                page_idx=page_idx,
                                            )
                                        )

                    else:
                        lines_list = block.get('lines', []) if isinstance(block, dict) else []
                        block_index = block.get('index') if isinstance(block, dict) else None
                        block_type = block.get('type') if isinstance(block, dict) else None

                        for line_idx, line in enumerate(lines_list or []):
                            spans = line.get('spans') if isinstance(line, dict) else None
                            if not spans:
                                continue

                            # è¡¨æ ¼ï¼ˆå— target_types æ§åˆ¶ï¼‰
                            for s_idx, s in enumerate(spans):
                                if not isinstance(s, dict):
                                    continue
                                if s.get('type') == 'table' and 'bbox' in s and ('table' in target_types):
                                    targets.append(
                                        TargetInfo(
                                            id=f"page_{page_idx}_span_table_{block_index}",
                                            type='span_table',
                                            text=s.get('html', ''),
                                            bbox=s['bbox'],
                                            page_idx=page_idx,
                                        )
                                    )

                            # è¡Œé—´å…¬å¼ï¼ˆå— target_types æ§åˆ¶ï¼‰
                            for s_idx, s in enumerate(spans):
                                if not isinstance(s, dict):
                                    continue
                                if s.get('type') == 'interline_equation' and 'bbox' in s and ('interline_equation' in target_types):
                                    targets.append(
                                        TargetInfo(
                                            id=f"page_{page_idx}_span_interline_equation_{block_index}",
                                            type='span_interline_equation',
                                            text=s.get('content', ''),
                                            bbox=s['bbox'],
                                            page_idx=page_idx,
                                        )
                                    )

                            # åˆå¹¶æ–‡æœ¬/è¡Œå†…å…¬å¼ï¼ˆä»…å½“å¯ç”¨ text æå–ï¼›æ ‡é¢˜ç”±ä¸Šå±‚ block ç±»å‹æ§åˆ¶ï¼‰
                            if 'text' in target_types:
                                parts = []
                                boxes = []
                                is_title = (block_type == 'title')
                                for s in spans:
                                    if not isinstance(s, dict):
                                        continue
                                    st = s.get('type', '')
                                    if st in ('text', 'inline_equation'):
                                        content = s.get('content', '')
                                        if st == 'inline_equation' and content:
                                            content = f"${content}$"
                                        if content:
                                            parts.append(content)
                                        if 'bbox' in s:
                                            boxes.append(s['bbox'])

                                if parts:
                                    merged = ''.join(parts)
                                    if boxes:
                                        xs1 = [b[0] for b in boxes]
                                        ys1 = [b[1] for b in boxes]
                                        xs2 = [b[2] for b in boxes]
                                        ys2 = [b[3] for b in boxes]
                                        mbox = [min(xs1), min(ys1), max(xs2), max(ys2)]
                                    else:
                                        mbox = spans[0].get('bbox', [0, 0, 0, 0])

                                    out_type = 'title_text' if is_title else 'text'
                                    out_text = f"# {merged}" if is_title else merged
                                    targets.append(
                                        TargetInfo(
                                            id=f"page_{page_idx}_line_text_{block_index}",
                                            type=out_type,
                                            text=out_text,
                                            bbox=mbox,
                                            page_idx=page_idx,
                                        )
                                    )

        logger.info(f"ä»JSONä¸­æå–äº† {len(targets)} ä¸ªç›®æ ‡")
        return targets

    except Exception as e:
        logger.error(f"æå–ç›®æ ‡ä¿¡æ¯å¤±è´¥: {e}")
        return []


def find_pdf_file_v2(book_name: str, base_folder: str) -> Optional[str]:
    """æŸ¥æ‰¾PDFæ–‡ä»¶ï¼šä¼˜å…ˆä½¿ç”¨ä¸€æ¬¡æ€§æ„å»ºçš„ç¼“å­˜ï¼›å¿…è¦æ—¶å›é€€åˆ°éå†ã€‚"""
    if not os.path.exists(base_folder):
        logger.warning(f"æœç´¢åŸºç¡€æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {base_folder}")
        return None

    key = _stem_key(book_name)

    # 1) ä¼˜å…ˆæŸ¥å…¨å±€ç¼“å­˜
    if PDF_CACHE:
        path = PDF_CACHE.get(key)
        if path and os.path.exists(path):
            return path

    # 2) å›é€€ï¼šéå†ï¼ˆè§„èŒƒåŒ–æ¯”è¾ƒï¼‰
    try:
        for root, dirs, files in os.walk(base_folder):
            for filename in files:
                if not filename.lower().endswith('.pdf'):
                    continue
                name_without_ext = os.path.splitext(filename)[0]
                name_key = _stem_key(name_without_ext)

                if name_key == key:
                    pdf_path = os.path.join(root, filename)
                    logger.info(f"æ‰¾åˆ°PDFæ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {book_name} -> {pdf_path}")
                    return pdf_path
    except Exception as e:
        logger.error(f"æœç´¢PDFæ–‡ä»¶å‡ºé”™: {e}")

    logger.warning(f"æœªæ‰¾åˆ°PDFæ–‡ä»¶: {book_name}")
    return None


def build_pdf_cache(base_folder: str, target_books: list = None) -> dict:
    """æ„å»ºPDFæ–‡ä»¶ç¼“å­˜ï¼Œä¸€æ¬¡æ€§æœç´¢æ‰€æœ‰PDFæ–‡ä»¶"""
    pdf_cache: dict[str, str] = {}
    
    if not os.path.exists(base_folder):
        logger.warning(f"æœç´¢åŸºç¡€æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {base_folder}")
        return pdf_cache
    
    try:
        # å¦‚æœæä¾›äº†ç›®æ ‡ä¹¦ç±åˆ—è¡¨ï¼Œå…ˆå°è¯•ç²¾ç¡®æœç´¢
        if target_books:
            logger.info(f"å¼€å§‹ä¸º {len(target_books)} æœ¬ç›®æ ‡ä¹¦ç±æ„å»ºPDFç¼“å­˜ï¼Œæœç´¢ç›®å½•: {base_folder}")
            
            with tqdm(target_books, desc="ğŸ” æœç´¢ç›®æ ‡ä¹¦ç±PDF", unit="æœ¬") as pbar:
                for book_name in pbar:
                    pbar.set_description(f"ğŸ” æœç´¢: {book_name[:20]}...")
                    
                    patterns = [
                        f"{book_name}.pdf"
                    ]
                    
                    found_for_book = False
                    for root, dirs, files in os.walk(base_folder):
                        if found_for_book:
                            break
                            
                        for pattern in patterns:
                            for filename in files:
                                if filename.lower().endswith('.pdf') and fnmatch.fnmatch(filename, pattern):
                                    pdf_path = os.path.join(root, filename)
                                    name_without_ext = os.path.splitext(filename)[0]
                                    
                                    # å­˜å‚¨stemé”®
                                    pdf_cache[_stem_key(name_without_ext)] = pdf_path
                                    
                                    pbar.set_description(f"âœ… æ‰¾åˆ°: {book_name[:20]}...")
                                    found_for_book = True
                                    break
                            if found_for_book:
                                break
            
            found_books = len([book for book in target_books if book in pdf_cache])
            logger.info(f"ç²¾ç¡®æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {found_books}/{len(target_books)} æœ¬ç›®æ ‡ä¹¦ç±çš„PDFæ–‡ä»¶")
    except Exception as e:
        logger.error(f"æ„å»ºPDFç¼“å­˜å‡ºé”™: {e}")
        return pdf_cache



def check_book_already_processed(book_name: str, output_base_dir: str) -> bool:
    """æ£€æŸ¥ä¹¦ç±æ˜¯å¦å·²ç»å¤„ç†è¿‡"""
    try:
        book_output_dir = os.path.join(output_base_dir, book_name)
        
        if not os.path.exists(book_output_dir):
            return False
        
        result_json_path = os.path.join(book_output_dir, "extraction_results.json")
        if not os.path.exists(result_json_path):
            return False
        
        # ç»Ÿè®¡å„åˆ†ç±»ç›®å½•ä¸‹çš„ png
        categories = [
            os.path.join(book_output_dir, "table_images"),
            os.path.join(book_output_dir, "equation_images"),
            os.path.join(book_output_dir, "text_images"),
            os.path.join(book_output_dir, "images"),  # å…¼å®¹æ—§ç»“æ„/æœªçŸ¥ç±»å‹
        ]
        num_png = 0
        for cat in categories:
            if not os.path.exists(cat):
                continue
            for root, _, files in os.walk(cat):
                num_png += sum(1 for f in files if f.lower().endswith('.png'))
        if num_png == 0:
            return False
        
        logger.debug(f"ä¹¦ç± {book_name} å·²ç»å¤„ç†è¿‡ï¼Œæœ‰ {num_png} å¼ å›¾ç‰‡")
        return True
        
    except Exception as e:
        logger.warning(f"æ£€æŸ¥ä¹¦ç± {book_name} å¤„ç†çŠ¶æ€æ—¶å‡ºé”™: {e}")
        return False


def get_processed_books_from_filesystem(output_base_dir: str) -> set:
    """ä»æ–‡ä»¶ç³»ç»Ÿè·å–å·²å¤„ç†çš„ä¹¦ç±åˆ—è¡¨"""
    processed_books = set()
    
    try:
        if not os.path.exists(output_base_dir):
            return processed_books
        
        for item in os.listdir(output_base_dir):
            item_path = os.path.join(output_base_dir, item)
            if os.path.isdir(item_path):
                if check_book_already_processed(item, output_base_dir):
                    processed_books.add(item)
        
        logger.info(f"ä»æ–‡ä»¶ç³»ç»Ÿå‘ç° {len(processed_books)} æœ¬å·²å¤„ç†çš„ä¹¦ç±")
        
    except Exception as e:
        logger.error(f"ä»æ–‡ä»¶ç³»ç»Ÿè·å–å·²å¤„ç†ä¹¦ç±åˆ—è¡¨å¤±è´¥: {e}")
    
    return processed_books


def process_pdf_extraction(json_path: str, pdf_path: Optional[str] = None, output_dir: Optional[str] = None, include_text: bool = False) -> ProcessResult:
    """å¤„ç†PDFå›¾åƒæˆªå–"""
    try:
        # è§£æJSONæ–‡ä»¶
        json_data = parse_json_file(json_path)
        
        # ç¡®å®šPDFè·¯å¾„
        if not pdf_path:
            if 'pdf_path' in json_data:
                pdf_path = json_data['pdf_path']
            else:
                json_file = Path(json_path)
                pdf_path = str(json_file.parent / f"{json_file.stem}.pdf")
        
        if not os.path.exists(pdf_path):
            raise Exception(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        
        # ç¡®å®šä¹¦ç±åç§°
        pdf_basename = os.path.basename(pdf_path)
        book_name = os.path.splitext(pdf_basename)[0]
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if not output_dir or output_dir.strip() == "":
            base_output_dir = str(Path(pdf_path).parent)
        else:
            if not os.path.isabs(output_dir):
                base_output_dir = str(Path.cwd() / output_dir)
            else:
                base_output_dir = output_dir
        
        # åˆ›å»ºä¹¦ç±ä¸“ç”¨ç›®å½•
        book_output_dir = os.path.join(base_output_dir, book_name)
        os.makedirs(book_output_dir, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶åˆ°ä¹¦ç±ç›®å½•
        #pdf_copy_path = os.path.join(book_output_dir, pdf_basename)
        #if not os.path.exists(pdf_copy_path):
        #    shutil.copy2(pdf_path, pdf_copy_path)
        
        json_basename = os.path.basename(json_path)
        json_copy_path = os.path.join(book_output_dir, json_basename)
        if not os.path.exists(json_copy_path):
            shutil.copy2(json_path, json_copy_path)
        
        # æå–ç›®æ ‡ä¿¡æ¯
        targets = extract_targets_from_json(json_data, include_text=include_text)
        
        if not targets:
            return ProcessResult(
                success=False,
                message="æœªæ‰¾åˆ°å¯å¤„ç†çš„ç›®æ ‡",
                saved_images=[],
                total_targets=0,
                processed_targets=0
            )
        
        # åˆ›å»ºPDFå›¾åƒæˆªå–å™¨
        extractor = PDFImageExtractor(pdf_path)
        
        saved_images = []
        processed_count = 0
        # åˆ†ç±»å­ç›®å½•
        table_dir = os.path.join(book_output_dir, "table_images")
        equation_dir = os.path.join(book_output_dir, "equation_images") 
        text_dir = os.path.join(book_output_dir, "text_images")
        os.makedirs(table_dir, exist_ok=True)
        os.makedirs(equation_dir, exist_ok=True)
        os.makedirs(text_dir, exist_ok=True)
        # id åˆ°å›¾ç‰‡è·¯å¾„çš„æ˜ å°„
        id_to_image_path: dict[str, str] = {}
        
        try:
            # å¤„ç†æ¯ä¸ªç›®æ ‡ï¼ˆä¸æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œé¿å…åœ¨å¤šè¿›ç¨‹ä¸­æ··ä¹±ï¼‰
            for target in targets:
                # æ ¹æ®ç±»å‹é€‰æ‹©åˆ†ç±»ç›®å½•
                if target.type == 'span_table':
                    target_dir = table_dir
                elif target.type == 'span_interline_equation':
                    target_dir = equation_dir
                elif target.type in ('text', 'title_text'):
                    target_dir = text_dir
                else:
                    target_dir = os.path.join(book_output_dir, "images")

                output_path = os.path.join(target_dir, f"{target.id}.png")
                if extractor.extract_image_by_bbox(target.page_idx, target.bbox, output_path):
                    saved_images.append(output_path)
                    id_to_image_path[target.id] = output_path
                    processed_count += 1
                    logger.debug(f"æˆåŠŸæˆªå–: {target.type}#{target.page_idx}")
                else:
                    logger.warning(f"æˆªå–å¤±è´¥: {target.type}#{target.page_idx}")
            
        finally:
            extractor.close()
        
        # ä¿å­˜ç»“æœJSONï¼ˆæ€»æ±‡æ€» + æŒ‰ç±»å‹æ‹†åˆ†ï¼‰
        # result_json_path = os.path.join(book_output_dir, "extraction_results.json")
        table_json_path = os.path.join(book_output_dir, "table.json")
        equation_json_path = os.path.join(book_output_dir, "equation.json")
        text_json_path = os.path.join(book_output_dir, "text.json")
        try:
            result_data = {
                "pdf_path": pdf_path,
                "json_source": json_path,
                "extraction_time": str(time.time()),
                "total_targets": len(targets),
                "processed_targets": processed_count,
                "targets": []
            }
            
            for target in targets:
                # ä½¿ç”¨å®é™…ä¿å­˜æ—¶è®°å½•çš„è·¯å¾„ï¼Œè‹¥ä¸å­˜åœ¨åˆ™ä¸ºç©º
                img_path = id_to_image_path.get(target.id)
                target_info = {
                    "id": target.id,
                    "type": target.type,
                    "text": target.text,
                    "bbox": target.bbox,
                    "page_idx": target.page_idx,
                    "image_path": img_path
                }
                result_data["targets"].append(target_info)
            
            # with open(result_json_path, 'w', encoding='utf-8') as f:
            #     json.dump(result_data, f, ensure_ascii=False, indent=2)

            # å†™å‡ºæ‹†åˆ†åçš„ JSON
            def _filter_targets(tt: str) -> list[dict]:
                return [t for t in result_data["targets"] if t.get("type") == tt]

            table_payload = {
                "pdf_path": pdf_path,
                "json_source": json_path,
                "extraction_time": result_data["extraction_time"],
                "total_targets": len(_filter_targets("span_table")),
                "processed_targets": len(_filter_targets("span_table")),
                "targets": _filter_targets("span_table"),
            }
            equation_payload = {
                "pdf_path": pdf_path,
                "json_source": json_path,
                "extraction_time": result_data["extraction_time"],
                "total_targets": len(_filter_targets("span_interline_equation")),
                "processed_targets": len(_filter_targets("span_interline_equation")),
                "targets": _filter_targets("span_interline_equation"),
            }
            text_targets = [t for t in result_data["targets"] if t.get("type") in ("text", "title_text")]
            text_payload = {
                "pdf_path": pdf_path,
                "json_source": json_path,
                "extraction_time": result_data["extraction_time"],
                "total_targets": len(text_targets),
                "processed_targets": len(text_targets),
                "targets": text_targets,
            }

            with open(table_json_path, 'w', encoding='utf-8') as f:
                json.dump(table_payload, f, ensure_ascii=False, indent=2)
            with open(equation_json_path, 'w', encoding='utf-8') as f:
                json.dump(equation_payload, f, ensure_ascii=False, indent=2)
            if include_text and text_targets:
                with open(text_json_path, 'w', encoding='utf-8') as f:
                    json.dump(text_payload, f, ensure_ascii=False, indent=2)

            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {result_json_path}")
            logger.info(f"æ‹†åˆ†ç»“æœå·²ä¿å­˜åˆ°: {table_json_path}, {equation_json_path}{', ' + text_json_path if include_text and text_targets else ''}")
            
        except Exception as e:
            logger.warning(f"ä¿å­˜ç»“æœJSONå¤±è´¥: {e}")
        
        return ProcessResult(
            success=True,
            message=f"æˆåŠŸå¤„ç† {processed_count}/{len(targets)} ä¸ªç›®æ ‡ï¼Œè¾“å‡ºç›®å½•: {book_output_dir}",
            saved_images=saved_images,
            total_targets=len(targets),
            processed_targets=processed_count
        )
        
    except Exception as e:
        logger.error(f"å¤„ç†PDFå›¾åƒæˆªå–å¤±è´¥: {e}")
        return ProcessResult(
            success=False,
            message=f"å¤„ç†å¤±è´¥: {str(e)}",
            saved_images=[],
            total_targets=0,
            processed_targets=0
        )


def process_single_book_worker(book_task: BookTask) -> Dict:
    """å•ä¸ªä¹¦ç±å¤„ç†å·¥ä½œå‡½æ•°ï¼ˆå¤šè¿›ç¨‹ï¼‰"""
    start_time = time.time()
    try:
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†ä¹¦ç±: {book_task.book_name}")
        
        # æŒ‰éœ€æŸ¥æ‰¾PDFæ–‡ä»¶
        pdf_path = book_task.pdf_path
        if pdf_path is None and book_task.pdf_base_folder:
            logger.info(f"ğŸ” ä¸ºä¹¦ç± {book_task.book_name} æœç´¢PDFæ–‡ä»¶...")
            pdf_path = find_pdf_file_v2(book_task.book_name, book_task.pdf_base_folder)
            
            if pdf_path is None:
                logger.warning(f"âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶: {book_task.book_name}")
                return {
                    "book_name": book_task.book_name,
                    "success": False,
                    "message": "æœªæ‰¾åˆ°å¯¹åº”çš„PDFæ–‡ä»¶",
                    "json_files_found": 1,
                    "targets_processed": 0,
                    "images_saved": 0
                }
            
            logger.info(f"âœ… æ‰¾åˆ°PDFæ–‡ä»¶: {os.path.basename(pdf_path)}")
        
        # æ‰§è¡ŒPDFå›¾åƒæå–
        logger.info(f"ğŸ¯ å¼€å§‹æˆªå–å›¾åƒ: {book_task.book_name}")
        result = process_pdf_extraction(
            book_task.json_file_path,
            pdf_path,
            book_task.output_dir,
            include_text=book_task.include_text,
        )
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        elapsed_time = time.time() - start_time
        
        if result.success:
            logger.info(f"âœ… ä¹¦ç±å¤„ç†æˆåŠŸ: {book_task.book_name} - {result.processed_targets}ç›®æ ‡/{len(result.saved_images)}å›¾ç‰‡ ({elapsed_time:.1f}ç§’)")
        else:
            logger.warning(f"âŒ ä¹¦ç±å¤„ç†å¤±è´¥: {book_task.book_name} - {result.message}")
        
        return {
            "book_name": book_task.book_name,
            "success": result.success,
            "message": result.message,
            "json_files_found": 1,
            "targets_processed": result.processed_targets,
            "images_saved": len(result.saved_images),
            "processing_time": elapsed_time
        }
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"ğŸ’¥ å¤„ç†ä¹¦ç±å¼‚å¸¸: {book_task.book_name} - {str(e)} ({elapsed_time:.1f}ç§’)")
        return {
            "book_name": book_task.book_name,
            "success": False,
            "message": f"å¤„ç†å¼‚å¸¸: {str(e)}",
            "json_files_found": 0,
            "targets_processed": 0,
            "images_saved": 0,
            "processing_time": elapsed_time
        }


def batch_process_books(
    results_folder: str, 
    pdf_base_folder: str, 
    output_base_dir: Optional[str] = None,
    max_workers: Optional[int] = None,
    include_text: bool = False,
) -> BatchResult:
    """æ‰¹é‡å¤„ç†PDFå›¾åƒæˆªå–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    try:
        # éªŒè¯æ–‡ä»¶å¤¹
        if not os.path.exists(results_folder):
            raise Exception(f"ç»“æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {results_folder}")
        if not os.path.exists(pdf_base_folder):
            raise Exception(f"PDFåŸºç¡€æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {pdf_base_folder}")
        
        # ç¡®å®šè¾“å‡ºç›®å½•ï¼Œå¢åŠ æ€»æ–‡ä»¶å¤¹å±‚çº§
        if not output_base_dir:
            output_base_dir = os.path.join(results_folder, "batch_output")
        
        # åˆ›å»ºæ€»æ–‡ä»¶å¤¹ï¼šä½¿ç”¨results_folderçš„çˆ¶ç›®å½•æ–‡ä»¶å
        results_folder_path = Path(results_folder)
        total_folder_name = results_folder_path.parent.name
        if not total_folder_name or total_folder_name == ".":
            # å¦‚æœçˆ¶ç›®å½•æ˜¯æ ¹ç›®å½•ï¼Œä½¿ç”¨results_folderæœ¬èº«çš„åå­—
            total_folder_name = results_folder_path.name
        
        # æœ€ç»ˆè¾“å‡ºç›®å½•ç»“æ„ï¼šoutput_base_dir/æ€»æ–‡ä»¶å¤¹/
        final_output_dir = os.path.join(output_base_dir, total_folder_name)
        os.makedirs(final_output_dir, exist_ok=True)
        
        logger.info(f"è¾“å‡ºç›®å½•ç»“æ„: {output_base_dir}/{total_folder_name}/å„ä¹¦ç±æ–‡ä»¶å¤¹")
        logger.info(f"æ€»æ–‡ä»¶å¤¹å: {total_folder_name} (æ¥è‡ª: {results_folder_path.parent})")
        
        # æ›´æ–°output_base_dirä¸ºåŒ…å«æ€»æ–‡ä»¶å¤¹çš„è·¯å¾„
        output_base_dir = final_output_dir
        
        # è·å–æ‰€æœ‰ä¹¦ç±æ–‡ä»¶å¤¹
        book_folders = []
        for item in os.listdir(results_folder):
            item_path = os.path.join(results_folder, item)
            if os.path.isdir(item_path):
                book_folders.append(item)
        
        if not book_folders:
            raise Exception(f"åœ¨æ–‡ä»¶å¤¹ {results_folder} ä¸­æœªæ‰¾åˆ°ä»»ä½•ä¹¦ç±æ–‡ä»¶å¤¹")
        
        logger.info(f"æ‰¾åˆ° {len(book_folders)} ä¸ªä¹¦ç±æ–‡ä»¶å¤¹")
        
        # æ™ºèƒ½æ¢å¤ï¼šæ£€æŸ¥å·²å¤„ç†çš„ä¹¦ç±
        processed_books = get_processed_books_from_filesystem(output_base_dir)
        pending_books = [book_name for book_name in book_folders if book_name not in processed_books]
        
        logger.info(f"æ€»è®¡ {len(book_folders)} æœ¬ä¹¦ç±ï¼Œå·²å¤„ç† {len(processed_books)} æœ¬ï¼Œå¾…å¤„ç† {len(pending_books)} æœ¬")
        
        if processed_books:
            logger.info(f"è·³è¿‡ {len(processed_books)} æœ¬å·²å¤„ç†çš„ä¹¦ç±")
        
        # éªŒè¯JSONæ–‡ä»¶
        valid_books = []
        with tqdm(pending_books, desc="ğŸ“‹ éªŒè¯JSONæ–‡ä»¶", unit="æœ¬") as pbar:
            for book_name in pbar:
                pbar.set_description(f"ğŸ“‹ éªŒè¯: {book_name[:20]}...")
                
                book_folder_path = os.path.join(results_folder, book_name)
                json_file_pattern = f"{book_name}_middle.json"
                json_file_path = os.path.join(book_folder_path, json_file_pattern)
                
                if os.path.exists(json_file_path):
                    valid_books.append((book_name, json_file_path))
                    pbar.set_description(f"âœ… æœ‰æ•ˆ: {book_name[:20]}...")
                else:
                    pbar.set_description(f"âŒ è·³è¿‡: {book_name[:20]}...")
        
        logger.info(f"JSONéªŒè¯å®Œæˆï¼Œ{len(valid_books)} æœ¬ä¹¦ç±å‡†å¤‡å¤„ç†")
        
        # åœ¨å¤„ç†å‰æ„å»ºæˆ–æ›´æ–°ä¸€æ¬¡æ€§PDFç¼“å­˜ï¼ˆé’ˆå¯¹å¾…å¤„ç†ä¹¦ç±ï¼Œå¯å¿«é€Ÿå‘½ä¸­ï¼‰
        try:
            logger.info("æ„å»ºPDFæ–‡ä»¶ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§ï¼‰...")
            # ä¼˜å…ˆç”¨å¾…å¤„ç†ä¹¦ç±åˆ—è¡¨ä½œä¸ºç›®æ ‡ï¼Œä»¥åŠ é€Ÿç¼“å­˜æ„å»º
            cache = build_pdf_cache(pdf_base_folder, target_books=pending_books)
            # è‹¥å‘½ä¸­ç‡è¾ƒä½ï¼Œbuild_pdf_cache ä¼šè‡ªåŠ¨åšå…¨é‡æ‰«æ
            global PDF_CACHE
            PDF_CACHE = cache
            logger.info(f"PDFç¼“å­˜æ¡ç›®æ•°: {len(PDF_CACHE)}")
        except Exception as e:
            logger.warning(f"æ„å»ºPDFç¼“å­˜å¤±è´¥ï¼Œåç»­å°†å›é€€é€æœ¬æŸ¥æ‰¾: {e}")

        # åˆ›å»ºä»»åŠ¡
        book_tasks = []
        for book_name, json_file_path in valid_books:
            book_task = BookTask(
                book_name=book_name,
                json_file_path=json_file_path,
                pdf_path=None,
                output_dir=output_base_dir,
                task_id="batch_task",
                pdf_base_folder=pdf_base_folder,
                include_text=include_text,
            )
            book_tasks.append(book_task)
        
        # ä¸ºå·²å¤„ç†çš„ä¹¦ç±åˆ›å»ºç»“æœè®°å½•
        all_results = []
        for book_name in processed_books:
            # ç»Ÿè®¡åˆ†ç±»ç›®å½•ä¸‹çš„å›¾ç‰‡æ•°é‡
            base_dir = os.path.join(output_base_dir, book_name)
            cat_dirs = [
                os.path.join(base_dir, "table_images"),
                os.path.join(base_dir, "equation_images"),
                os.path.join(base_dir, "text_images"),
                os.path.join(base_dir, "images"),
            ]
            img_count = 0
            for cat in cat_dirs:
                if not os.path.exists(cat):
                    continue
                for _, _, files in os.walk(cat):
                    img_count += sum(1 for f in files if f.lower().endswith('.png'))

            all_results.append({
                "book_name": book_name,
                "success": True,
                "message": "æ™ºèƒ½æ¢å¤ï¼šè·³è¿‡å·²å¤„ç†æ–‡ä»¶",
                "json_files_found": 1,
                "targets_processed": 0,
                "images_saved": img_count
            })
        
        if not book_tasks:
            if processed_books:
                return BatchResult(
                    success=True,
                    message=f"æ‰€æœ‰ {len(processed_books)} æœ¬ä¹¦ç±å·²å¤„ç†å®Œæˆ",
                    total_books=len(book_folders),
                    processed_books=len(processed_books),
                    failed_books=[],
                    results=all_results
                )
            else:
                raise Exception("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„ä¹¦ç±")
        
        use_workers = MAX_WORKERS if (max_workers is None or max_workers <= 0) else max(1, max_workers)
        logger.info(f"å‡†å¤‡å¤„ç† {len(book_tasks)} æœ¬ä¹¦ç±ï¼Œä½¿ç”¨ {use_workers} ä¸ªè¿›ç¨‹")
        
        # å¤šè¿›ç¨‹å¤„ç†
        failed_books = []
        
        with ProcessPoolExecutor(max_workers=use_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_book = {
                executor.submit(process_single_book_worker, book_task): book_task
                for book_task in book_tasks
            }
            
            # è®¡ç®—æ€»çš„å¤„ç†è¿›åº¦ï¼ˆåŒ…æ‹¬å·²å¤„ç†çš„ä¹¦ç±ï¼‰
            total_books_to_process = len(book_folders)
            already_processed_count = len(processed_books)
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ•´ä½“å¤„ç†è¿›åº¦
            with tqdm(
                total=total_books_to_process, 
                initial=already_processed_count,  # ä»å·²å¤„ç†çš„ä¹¦ç±æ•°å¼€å§‹
                desc="ğŸ“š æ‰¹é‡å¤„ç†è¿›åº¦", 
                unit="æœ¬", 
                dynamic_ncols=True,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
            ) as pbar:
                
                # è®¾ç½®åˆå§‹çŠ¶æ€
                pbar.set_postfix({
                    'å·²è·³è¿‡': already_processed_count,
                    'å¾…å¤„ç†': len(book_tasks),
                    'æˆåŠŸ': 0,
                    'å¤±è´¥': 0
                })
                
                completed_count = 0  # æ–°å®Œæˆçš„ä»»åŠ¡æ•°
                
                for future in as_completed(future_to_book):
                    book_task = future_to_book[future]
                    try:
                        result = future.result()
                        all_results.append(result)
                        completed_count += 1
                        
                        if result["success"]:
                            status_msg = f"âœ… æˆåŠŸ: {result['book_name'][:20]}"
                            current_successful = len([r for r in all_results if r.get("success", False) and r.get("message") != "æ™ºèƒ½æ¢å¤ï¼šè·³è¿‡å·²å¤„ç†æ–‡ä»¶"])
                        else:
                            failed_books.append(result["book_name"])
                            status_msg = f"âŒ å¤±è´¥: {result['book_name'][:20]}"
                            current_successful = len([r for r in all_results if r.get("success", False) and r.get("message") != "æ™ºèƒ½æ¢å¤ï¼šè·³è¿‡å·²å¤„ç†æ–‡ä»¶"])
                        
                        # æ›´æ–°è¿›åº¦æ¡æè¿°å’Œåç¼€
                        pbar.set_description(f"ğŸ“š {status_msg}")
                        pbar.set_postfix({
                            'å·²è·³è¿‡': already_processed_count,
                            'æˆåŠŸ': current_successful,
                            'å¤±è´¥': len(failed_books),
                            'è¿›åº¦': f"{completed_count}/{len(book_tasks)}"
                        })
                        
                        # æ›´æ–°è¿›åº¦
                        pbar.update(1)
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                        targets = result.get("targets_processed", 0)
                        images = result.get("images_saved", 0)
                        logger.info(f"å®Œæˆå¤„ç† ({completed_count}/{len(book_tasks)}): {result['book_name']} - {targets}ç›®æ ‡/{images}å›¾ç‰‡")
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                        failed_books.append(book_task.book_name)
                        completed_count += 1
                        
                        error_result = {
                            "book_name": book_task.book_name,
                            "success": False,
                            "message": f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                            "json_files_found": 0,
                            "targets_processed": 0,
                            "images_saved": 0
                        }
                        all_results.append(error_result)
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.set_description(f"ğŸ“š ğŸ’¥ å¼‚å¸¸: {book_task.book_name[:20]}")
                        pbar.set_postfix({
                            'å·²è·³è¿‡': already_processed_count,
                            'æˆåŠŸ': len([r for r in all_results if r.get("success", False) and r.get("message") != "æ™ºèƒ½æ¢å¤ï¼šè·³è¿‡å·²å¤„ç†æ–‡ä»¶"]),
                            'å¤±è´¥': len(failed_books),
                            'è¿›åº¦': f"{completed_count}/{len(book_tasks)}"
                        })
                        pbar.update(1)
                
                # å®Œæˆåçš„æœ€ç»ˆçŠ¶æ€
                final_successful = len([r for r in all_results if r.get("success", False)])
                pbar.set_description(f"ğŸ“š ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ")
                pbar.set_postfix({
                    'æ€»æˆåŠŸ': final_successful,
                    'æ€»å¤±è´¥': len(failed_books),
                    'å®Œæˆåº¦': '100%'
                })
        
        # ç»Ÿè®¡ç»“æœ
        successful_books = len([r for r in all_results if r.get("success", False)])
        
        summary_message = f"æ‰¹é‡å¤„ç†å®Œæˆ: {successful_books}/{len(book_folders)} æœ¬ä¹¦ç±å¤„ç†æˆåŠŸ"
        if failed_books:
            summary_message += f"ï¼Œå¤±è´¥çš„ä¹¦ç±: {', '.join(failed_books)}"
        
        return BatchResult(
            success=successful_books > 0,
            message=summary_message,
            total_books=len(book_folders),
            processed_books=successful_books,
            failed_books=failed_books,
            results=all_results
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        return BatchResult(
            success=False,
            message=f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}",
            total_books=0,
            processed_books=0,
            failed_books=[],
            results=[]
        )


if __name__ == "__main__":
    # ç®€å•çš„å‘½ä»¤è¡Œæ¥å£
    import argparse
    
    parser = argparse.ArgumentParser(description="PDFå›¾åƒæˆªå–å™¨ - æ ¸å¿ƒåŠŸèƒ½")
    parser.add_argument("--results-folder", required=True, help="ç»“æœæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--pdf-base-folder", required=True, help="PDFæœç´¢åŸºç¡€ç›®å½•")
    parser.add_argument("--output-base-dir", help="è¾“å‡ºåŸºç¡€ç›®å½•")
    parser.add_argument("--max-workers", type=int, default=0, help="å¹¶å‘è¿›ç¨‹æ•°ï¼ˆé»˜è®¤0è¡¨ç¤ºè‡ªåŠ¨ï¼‰")
    parser.add_argument("--include-text", action="store_true", help="æ˜¯å¦åŒæ—¶æå–æ™®é€šæ–‡æœ¬(text)ç›®æ ‡")
    
    args = parser.parse_args()
    
    print("ğŸš€ PDFå›¾åƒæˆªå–å™¨ - æ‰¹é‡å¤„ç†å¼€å§‹")
    print("="*80)
    print(f"ğŸ“‚ ç»“æœæ–‡ä»¶å¤¹: {args.results_folder}")
    print(f"ğŸ“ PDFåŸºç¡€ç›®å½•: {args.pdf_base_folder}")
    print(f"ğŸ“¤ è¾“å‡ºç›®å½•: {args.output_base_dir or 'è‡ªåŠ¨åˆ›å»º'}")
    print("="*80)
    
    try:
        result = batch_process_books(
            results_folder=args.results_folder,
            pdf_base_folder=args.pdf_base_folder,
            output_base_dir=args.output_base_dir,
            max_workers=args.max_workers,
            include_text=args.include_text,
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*80)
        print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print("="*80)
        
        success_rate = (result.processed_books / result.total_books * 100) if result.total_books > 0 else 0
        
        print(f"ğŸ“Š å¤„ç†çŠ¶æ€: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
        print(f"ğŸ“š æ€»ä¹¦ç±æ•°: {result.total_books}")
        print(f"âœ… æˆåŠŸå¤„ç†: {result.processed_books} ({success_rate:.1f}%)")
        print(f"âŒ å¤±è´¥æ•°é‡: {len(result.failed_books)}")
        
        if result.failed_books:
            print(f"ğŸ’” å¤±è´¥ä¹¦ç±: {', '.join(result.failed_books[:3])}")
            if len(result.failed_books) > 3:
                print(f"   ... ä»¥åŠå…¶ä»– {len(result.failed_books) - 3} æœ¬")
        
        print(f"ğŸ’¬ æ¶ˆæ¯: {result.message}")
        print("="*80)
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        if result.results:
            print("\nğŸ“‹ æœ€è¿‘å¤„ç†ç»“æœ:")
            recent_results = result.results[-5:] if len(result.results) > 5 else result.results
            for book_result in recent_results:
                status = "âœ…" if book_result.get("success", False) else "âŒ"
                book_name = book_result.get("book_name", "æœªçŸ¥")[:30]
                targets = book_result.get("targets_processed", 0)
                images = book_result.get("images_saved", 0)
                
                if book_result.get("success", False):
                    print(f"  {status} {book_name}: {targets}ç›®æ ‡/{images}å›¾ç‰‡")
                else:
                    message = book_result.get("message", "æ— æ¶ˆæ¯")[:50]
                    print(f"  {status} {book_name}: {message}")
            
            if len(result.results) > 5:
                print(f"  ğŸ“ ... ä»¥åŠå…¶ä»– {len(result.results) - 5} ä¸ªç»“æœ")
        
        print("\nğŸš€ æ‰¹é‡å¤„ç†æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
